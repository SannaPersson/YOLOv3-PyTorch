{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "explicit-netscape",
   "metadata": {},
   "source": [
    "# Implementing and training YOLOv3 from scratch in PyTorch\n",
    "\n",
    "\n",
    "For such a popular paper there are still few implementations explained of the YOLOv3 architecture completely from scratch. I'll do my best to add something useful to the list. The code is written together with [Aladdin Persson](https://www.youtube.com/channel/UCkzW5JSFwvKRjXABI-UTAkQ) and can be found on [github](https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/object_detection/YOLOv3). You can also download pretrained weights on the Pascal-VOC that obtain 78.1 MAP [here](https://www.kaggle.com/sannapersson/yolov3-weights-for-pascal-voc-with-781-map) for the implementation below.  \n",
    "\n",
    "### Prerequisites:\n",
    "* Understanding the major parts in YOLOv1\n",
    "* Coding in PyTorch\n",
    "* Familiarity with convolutional networks and their training\n",
    "\n",
    "With this article I hope to convey:\n",
    "* Understanding of the key ideas necessary for implementing and training YOLOv3 from scratch in PyTorch\n",
    "* Complete code to use for training of YOLOv3\n",
    "* The relevant details of the algorithm to succeed if you choose to make you own implementation of YOLOv3\n",
    "\n",
    "The code is completely runnable if you download a utils.py and config.py file from the github above containing a few supporting functions and constants not specific to the YOLOv3 model. \n",
    "\n",
    "_Disclaimer: there are minor differences between this implementation and the original and I will point them out when we get to them._\n",
    "\n",
    "## Understanding the model \n",
    "Let's begin by understanding the fundamentals of the model. The YOLO (You only look once) algorithm is based on the idea that we divide the image into a grid with side $S$. The grid size depends upon which YOLO version we are implementing as well as the input image size but the details be clearer when we implement it. Each grid cell is responsible for making predictions of bounding boxes. You may then wonder what happens if an object covers several grid cells, will all predict a bounding box for the object? YOLO solves this by making only the cell containing the object's midpoint responsible for predicting the bounding box. This means that only one grid is responsible for each object's bounding box in the image. One drawback of this is that there can only be on bounding box in each grid cell. In YOLOv2 and forward they mitigate the issue by the making several bounding box predictions in the same grid cell. They also introduce anchor boxes which is an idea also seen in previous object detection papers such as Faster RCNN. \n",
    "![alt text](images/yolo_ex.png \"Title\")\n",
    "\n",
    "An anchor box is essentially a set of a width and a height chosen to represent a segment of the training data. For example a standing rectangle may suit a human while a wide rectangle is a better fit for a car. Using anchor boxes is a way of encoding knowledge about the training data into the model to help the model make appropriate predictions. It has been discussed whether this is actually desirable and there are more recent end-to-end approaches where anchor boxes are not used. The questions is then how to choose the anchors and an early approach was to hand design the anchors boxes by studying the training data, however, the authors of YOLOv2 found that using K-means clustering to generate them yielded better results. The anchors are used to allow the model to anchor its prediction to a predetermined box. The model will thus predict how much the true bounding box is offset in comparison with the anchor. This is one of the major differences from the original YOLO model. Each grid cell will have several anchor boxes and each anchor box can make one bounding box prediction. Each bounding box prediction will also be coupled with an object score as well as class predictions. The object score should reflect product of the probability that there is an object in the bounding box and the intersection over union between the predicted bounding box and the actual object. That means if there is no object in the grid cell corresponding to the specific anchor the target is zero and otherwise it is the intersection over union between the predicted box and the target bounding box. \n",
    "\n",
    "The predictions from the model $t_i$ are offsets to the anchors and will be converted to bounding boxes according to the following equations\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "b_{x}=\\sigma\\left(t_{x}\\right)+c_{x} \\\\\n",
    "b_{y}=\\sigma\\left(t_{y}\\right)+c_{y} \\\\\n",
    "b_{w}=p_{w} e^{t_{w}} \\\\\n",
    "b_{h}=p_{h} e^{t_{h}}.\n",
    "\\end{array}\n",
    "$$\n",
    "where $p_w$ and $p_h$ are the width and height of the corresponding anchor boxes and $\\{b_x, b_y, b_w, b_h\\}$ is the resulting bounding box. \n",
    "\n",
    "In YOLOv3 the backbone network is DarkNet-53 and its structure can be understood from the following table. This network was pretrained on ImageNet and is used as a feature extractor in the YOLOv3 model. The paper, however, completely skips detailing the following 53 convolutional layers in the YOLOv3 model where the actual prediction of bounding boxes takes place in the model. \n",
    "![alt text](images/darknet_53_table.png \"Title\")\n",
    "\n",
    "The prediction of bounding boxes happens on three different places in the networks on three different scales. In this context a scale means the grid size, $S$, which we divide the image into. In YOLOv3 we predict bounding boxes on three different grid sizes. The intuition behind this is that on a coarser grid larger objects can more easily be detected and vice versa for smaller objects on finer grids. We therefore also divide the anchor boxes we have found such that we assign all the smallest anchors to the last and finest scale and the largest anchor boxes to the coarsest grid. In YOLOv3 the grid sizes used are [13, 26, 52] for an image size of 416x416. If you use another image size the first grid size will be the image size divided by 32 and the others will be a multiple of two of the previous one. The details of the model will be clear when we implement it but the following image by  [Ayoosh Kathuria](https://medium.com/@ayoosh) (check out his Medium) gives great insight into the model architecture.  \n",
    "\n",
    "![alt text](images/yolo_architecture.png \"Title\")\n",
    "\n",
    "The backbone network is a standard convolutional network quite similar to previous Darknet versions with the addition of residual connections. It is really after layer 53 that the interesting parts happen. As the architecture image visualizes there are three downward paths corresponding to predictions of three different grid scales. The network then continues forward from the place it was before the prediction path. After the first and second scale prediction paths there is an upscaling layer to double the size of the feature map and concatenates the feature mapes with a route from a previous layer along the channel dimension. The image details which convolutional layers the routes come from but we will instead use a trick to find them in our implementation.\n",
    "\n",
    "We are now ready to start actually coding the model. All model details are found in the configuration file for YOLOv3 on [Joseph Redmon's Github](https://github.com/pjreddie) who is the author of the paper. \n",
    "\n",
    "## Coding the model\n",
    "This is the part of the YOLOv3 implementation that I spent both the least and the most time on debugging. I found it manageable to make the model work but it took some time to correct details to make sure the original weights could be loaded.  \n",
    "Everything in this section will be in a model.py file on [Github](https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/object_detection/YOLOv3/model.py). Let's start with the imports:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-trail",
   "metadata": {},
   "source": [
    "First we will define the architecture building blocks in a list as a way of parsing the original config file that majorly increases the readibility and grasp of the complete model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Information about architecture config:\n",
    "Tuple is structured by and signifies a convolutional block (filters, kernel_size, stride) \n",
    "Every convolutional layer is a same convolution. \n",
    "List is structured by \"B\" indicating a residual block followed by the number of repeats. \n",
    "\"S\" is for a scale prediction block and computing the yolo loss\n",
    "\"U\" is for upsampling the feature map\n",
    "\"\"\"\n",
    "config = [\n",
    "    (32, 3, 1),\n",
    "    (64, 3, 2),\n",
    "    [\"B\", 1],\n",
    "    (128, 3, 2),\n",
    "    [\"B\", 2],\n",
    "    (256, 3, 2),\n",
    "    [\"B\", 8],\n",
    "    # first route from the end of the previous block\n",
    "    (512, 3, 2),\n",
    "    [\"B\", 8], \n",
    "    # second route from the end of the previous block\n",
    "    (1024, 3, 2),\n",
    "    [\"B\", 4],\n",
    "    # until here is YOLO-53\n",
    "    (512, 1, 1),\n",
    "    (1024, 3, 1),\n",
    "    \"S\",\n",
    "    (256, 1, 1),\n",
    "    \"U\",\n",
    "    (256, 1, 1),\n",
    "    (512, 3, 1),\n",
    "    \"S\",\n",
    "    (128, 1, 1),\n",
    "    \"U\",\n",
    "    (128, 1, 1),\n",
    "    (256, 3, 1),\n",
    "    \"S\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-guyana",
   "metadata": {},
   "source": [
    "#### Defining the building blocks\n",
    "We will now define the most common building blocks of the architecture as separate classes to avoid repeating code over and over again. Each tuple signifies a convolutional block with batch normalization and leaky relu added to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.1)\n",
    "        self.use_bn_act = bn_act\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_bn_act:\n",
    "            return self.leaky(self.bn(self.conv(x)))\n",
    "        else:\n",
    "            return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-bridges",
   "metadata": {},
   "source": [
    "This layer also allows us to toggle the `bn_act` to false and skip the batch normalization and activation function which we will use in the last layer before output. In the case where we use batch normalization the bias term of the convolutional layer will have to effect but occupying VRAM. \n",
    "\n",
    "We then define the residual block which is essentially a combination of two convolutional blocks with a residual connection. The number of channels will be halved in the first convolutional layer and then doubled again in the second. The input size will therefore be maintained through the residual block. As in the `CNNBlock` we will have an argument to allow us to skip the residual connection which we will use in parts of the architecture.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_residual=True, num_repeats=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for repeat in range(num_repeats):\n",
    "            self.layers += [\n",
    "                nn.Sequential(\n",
    "                    CNNBlock(channels, channels // 2, kernel_size=1),\n",
    "                    CNNBlock(channels // 2, channels, kernel_size=3, padding=1),\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        self.use_residual = use_residual\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x) + self.use_residual * x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-tribune",
   "metadata": {},
   "source": [
    "The last predefined block we will use is the ScalePrediction which is the last two convolutional layers leading up to the prediction for each scale. Here the image of the architecture above actually is slightly incorrect and this block includes the downward path except for the loss function. We will reshape the output such that it has the the shape (batch size, anchors per scale, grid size, grid size, 5 + number of classes) where 5 refers to the object score and four bounding box coordinates. To obtain this shape we have to permute the output such that the class predictions end up in the last dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalePrediction(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, anchors_per_scale):\n",
    "        super(ScalePrediction, self).__init__()\n",
    "        self.pred = nn.Sequential(\n",
    "            CNNBlock(in_channels, 2*in_channels, kernel_size=3, padding=1),\n",
    "            CNNBlock(2*in_channels, (num_classes + 5) * 3, bn_act=False, kernel_size=1),\n",
    "        )\n",
    "        self.num_classes = num_classes\n",
    "        self.anchors_per_scale = anchors_per_scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            self.pred(x)\n",
    "                .reshape(x.shape[0], self.anchors_per_scale, self.num_classes + 5, x.shape[2], x.shape[3])\n",
    "                .permute(0, 1, 3, 4, 2)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-township",
   "metadata": {},
   "source": [
    "### Putting it together in YOLOv3\n",
    "We will now put it all together to the YOLOv3 model for the detection task. Most of the action takes place in the `_create_conv_layers` function where we build the model using the blocks defined above. Essentially we will just loop through the config list that we created above and add the blocks defined above in the correct order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv3(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=80):\n",
    "        super(YOLOv3, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.layers = self._create_conv_layers()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        route_connections = []\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ScalePrediction):\n",
    "                outputs.append(layer(x))\n",
    "                continue\n",
    "\n",
    "            x = layer(x)\n",
    "\n",
    "            if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
    "                route_connections.append(x)\n",
    "\n",
    "            elif isinstance(layer, nn.Upsample):\n",
    "                x = torch.cat([x, route_connections[-1]], dim=1)\n",
    "                route_connections.pop()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def _create_conv_layers(self):\n",
    "        layers = nn.ModuleList()\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for module in config:\n",
    "            if isinstance(module, tuple):\n",
    "                out_channels, kernel_size, stride = module\n",
    "                layers.append(\n",
    "                    CNNBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        padding=1 if kernel_size == 3 else 0,\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "\n",
    "            elif isinstance(module, list):\n",
    "                num_repeats = module[1]\n",
    "                layers.append(\n",
    "                    ResidualBlock(\n",
    "                        in_channels,\n",
    "                        num_repeats=num_repeats,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            elif isinstance(module, str):\n",
    "                if module == \"S\":\n",
    "                    layers += [\n",
    "                        ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n",
    "                        CNNBlock(in_channels, in_channels // 2, kernel_size=1),\n",
    "                        ScalePrediction(in_channels // 2, num_classes=self.num_classes),\n",
    "                    ]\n",
    "                    in_channels = in_channels // 2\n",
    "\n",
    "                elif module == \"U\":\n",
    "                    layers.append(\n",
    "                        nn.Upsample(scale_factor=2),\n",
    "                    )\n",
    "                    in_channels = in_channels * 3\n",
    "\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-greene",
   "metadata": {},
   "source": [
    "The trickiest part here is in the case in where there is an `\"S\"` in the config list which means that we are on the last layers towards a prediction on a specific scale. In these cases we will have three convolutional layers (one residual block and one convolutional block) following the same pattern on all prediction scales. To avoid creating a mess in the config list it is easiest to just add them here before the ScalePrediction. \n",
    "\n",
    "It should also be noted that we triple the `in_channels` after we add the upsamling layer and this is due to the route that we will concatenate in the forward propagation that has twice as many channels as the output from the upsampling layer. \n",
    "\n",
    "This leads us into the structure of the forward function. In the first if statement we check if the layer is a `ScalePrediction` block and in this case we will append its output to a list and later on compute the loss for each of the predictions separetely. We will then continue on in the model from the place the Scaleprediction started.\n",
    "\n",
    "I earlier mentioned that we will use a trick to find the layers that are routed forward. The second if-statement will take care of this and find the route layers specified in the image of the architecture above without us keeping track of unnecessarily complicated indices. The two routes will be the outputs from the residual blocks in the config list which have 8 repeats which we found this by just reading the original model configuration carefully. When we encounter an upsamling layer we will concatenate the output with the last route previously found following the image of the architecture above. \n",
    "\n",
    "Before we move on to the data loading I'll add a test function below that acts as a sanity check that the model at least outputs the correct shapes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    num_classes = 20\n",
    "    model = YOLOv3(num_classes=num_classes)\n",
    "    img_size = 416\n",
    "    x = torch.randn((2, 3, img_size, img_size))\n",
    "    out = model(x)\n",
    "    assert out[0].shape == (2, 3, img_size//32, img_size//32, 5 + num_classes)\n",
    "    assert out[1].shape == (2, 3, img_size//16, img_size//16, 5 + num_classes)\n",
    "    assert out[2].shape == (2, 3, img_size//8, img_size//8, 5 + num_classes)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-helena",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "In the dataset class we will load an image and the corresponding bounding boxes, perform augmentation using the [Albumentations library](https://albumentations.ai/) and then create the matrix form of the target that will be used to compute the loss. If you are not familiar with the Albumentations library it is a augmentations library with official support for PyTorch that can be used for data augmentation for detection, segmentation and other tasks which requires that the augmentations are performed both on the image and the target. \n",
    "\n",
    "We earlier mentioned that each scale will have anchor boxes associated with them and in the data loading we will compute which cell and which anchor that should be responsible for the particular target bounding box. Everything in this section will be in a dataset.py file. \n",
    "\n",
    "\n",
    "### Imports\n",
    "Most of the imports we are using are standard for the dataset class in PyTorch with the additional Albumentations package for the data augmentation. The imports from the utils, however, require some additional explanation. In a utils.py file that you can find on [Github](https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/object_detection/YOLOv3/utils.py) we will store some functions for handling bounding boxes conversions, [non-max suppression](https://youtu.be/YDkjWEN8jNA) and [mean average precision](https://youtu.be/FppOzcDvaDI). The only function that we will use in the data loading is the [intersection over union](https://www.youtube.com/watch?v=XXYG5ZWtjj0) function taking as input two tensors with the width and height of bounding boxes and outputting the corresponding intersection over union. The other files we import from the utils are only for checking that the data loading actually works. Plotting images and bounding boxes each time you modify the dataset class or augmentations can save you a lot of debugging time.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import (\n",
    "    cells_to_bboxes, # only for testing\n",
    "    iou_width_height as iou,\n",
    "    non_max_suppression as nms, # only for testing\n",
    "    plot_image #only for testing\n",
    ")\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-graphic",
   "metadata": {},
   "source": [
    "### Data format\n",
    "\n",
    "The part of the data loading that is different from image classification is the way we process the bounding boxes and format them such that they can be inputted to the model. The data loading below assumes that the data is formatted such that you have a folder with all images, a folder with a text file for each image detailing the bounding boxes and one or several csv files for the train, development and test set. The text file for an image should be formatted such that each row corresponds to a bounding box of the image with class label, x coordinate y coordinate, width, height in that specific order. The bounding box coordinates should be relative to the image such that if an object has midpoint in the middle of the image and covers it in half in both width and height we would specify: class label 0.5 0.5 0.5 0.5, on a row in the text file. In the csv file you want to specify the image file name and the text file name in two different columns. \n",
    "\n",
    "If you just want to get started without having to format the data you can download the Pascal-VOC dataset from Kaggle [here](https://www.kaggle.com/aladdinpersson/pascal-voc-yolo-works-with-albumentations) where the data is already formatted. \n",
    "\n",
    "Even if your dataset is not formatted this way it should be manageable to modify the data loading such that you can still make the training labels the same way. \n",
    "\n",
    "### Dataset class overview\n",
    "In a Pytorch dataset there are three building blocks: the init-method, the dataset length and the \\_\\_getitem\\_\\_-method. \n",
    "\n",
    "\n",
    "The important part in dataset class is how we handle the anchor boxes. We will specify the anchor boxes in the following manner\n",
    "```python\n",
    "ANCHORS = [\n",
    "    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
    "    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
    "    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
    "] \n",
    "```\n",
    "where each tuple corresponds to the width and the height of a anchor box relative to the image size and each list grouping together three tuples correspond to the anchors used on a specific prediction scale. The first list contains the largest anchor boxes which will be used for prediction on the coarsest grid where its presumably easier to predict larger bounding boxes. The following lists containing medium and small anchor boxes will be used for the medium and finest grid following the same reasoning. The anchors above are the ones used in the original paper but have beeen scaled to be relative to the image. \n",
    "\n",
    "Even if you are training on another dataset these anchors will probably work quite well, however, if your dataset is very different from MSCOCO you would probably generate your own anchor boxes and then it is probably wise to assign the anchor boxes to the different scales by their size as was done in the paper. In this case you would collect data of the widths and heights of the bounding boxes in your dataset and run these through K-means clustering with the intersection of union as the distance measure. The resulting centroids would be your anchor boxes. \n",
    "\n",
    "Below is the complete dataset class. We will load an image and its bounding boxes and perform augmentations on both. For each bounding box we will then assign it to the grid cell which contains its midpoint and decide which anchor is responsible for it by determining which anchor the bounding box has highest intersection over union with. Exactly how we build the targets is explained more in depth below the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a Pytorch dataset to load the Pascal VOC & MS COCO datasets\n",
    "\"\"\"\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_file,\n",
    "        img_dir,\n",
    "        label_dir,\n",
    "        anchors,\n",
    "        image_size=416,\n",
    "        S=[13, 26, 52],\n",
    "        C=20,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])  # for all 3 scales\n",
    "        self.num_anchors = self.anchors.shape[0]\n",
    "        self.num_anchors_per_scale = self.num_anchors // 3\n",
    "        self.C = C\n",
    "        self.ignore_iou_thresh = 0.5\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n",
    "        bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        # apply augmentations with albumentations \n",
    "        if self.transform:\n",
    "            augmentations = self.transform(image=image, bboxes=bboxes)\n",
    "            image = augmentations[\"image\"]\n",
    "            bboxes = augmentations[\"bboxes\"]\n",
    "       \n",
    "        # Building the targets below:\n",
    "        # Below assumes 3 scale predictions (as paper) and same num of anchors per scale\n",
    "        targets = [torch.zeros((self.num_anchors // 3, S, S, 6)) for S in self.S]\n",
    "        for box in bboxes:\n",
    "            iou_anchors = iou(torch.tensor(box[2:4]), self.anchors)\n",
    "            anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n",
    "            x, y, width, height, class_label = box\n",
    "            has_anchor = [False] * 3  # each scale should have one anchor\n",
    "            for anchor_idx in anchor_indices:\n",
    "                scale_idx = anchor_idx // self.num_anchors_per_scale\n",
    "                anchor_on_scale = anchor_idx % self.num_anchors_per_scale\n",
    "                S = self.S[scale_idx]\n",
    "                i, j = int(S * y), int(S * x)  # which cell\n",
    "                anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
    "                if not anchor_taken and not has_anchor[scale_idx]:\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
    "                    x_cell, y_cell = S * x - j, S * y - i  # both between [0,1]\n",
    "                    width_cell, height_cell = (\n",
    "                        width * S,\n",
    "                        height * S,\n",
    "                    )  # can be greater than 1 since it's relative to cell\n",
    "                    box_coordinates = torch.tensor(\n",
    "                        [x_cell, y_cell, width_cell, height_cell]\n",
    "                    )\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n",
    "                    has_anchor[scale_idx] = True\n",
    "\n",
    "                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = -1  # ignore prediction\n",
    "\n",
    "        return image, tuple(targets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-generator",
   "metadata": {},
   "source": [
    "In the init-metod we will just combine the list above to a tensor of shape (9,2) by `self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])` corresponding to each anchor box on all scales. We will also specify an ignore-threshold which will be used when building the targets as is explained below. \n",
    "\n",
    "The second challenging part of the data loading is in the getitem-method where we will load the image and the corresponding text file for the bounding boxes and process it such that we can input it to the model. For data augmentation we use the Albumentations library which requires the image and bounding boxes to be numpy arrays. The bounding boxes are also expected to be in the format \\[x, y, width, height, class label\\] which is different from how we have formatted it in the text file and we therefore use `np.roll` to change this. The reason for this inconsistency is that the text files are structured the same way as in the original implementation and if you are formatting a custom dataset you may consider modifying this if you are also using Albumentations. \n",
    "\n",
    "Here it should be noted that if you download Pascal-VOC or MS COCO dataset from the official sites or from Joseph Redmon's website you may run into some out of range issues when using Albumentations depending on how you convert the labels to the format x, y, width, height where (x,y) signifies the object's midpoint. If you do, make sure you have converted the labels as is specified in this [Github issue](https://github.com/albumentations-team/albumentations/issues/459) and you will save a couple of hours of debugging. \n",
    "\n",
    "### Building targets\n",
    "When we load the labels for a specific image it will only be an array with all the bounding boxes and to be able to calculate the loss we want to format the targets similarily to the model output. The model will output predictions on three different scales so we will also build three different targets. Each target for a particular scale and image will have shape (number of anchors // 3, grid size, grid size, 6) where 6 corresponds to the object score, four bounding box coordinates and class label. We make two assumptions which are that there is only one label per bounding box and that there is an equal number of bounding boxes on each scale. We start with initializing the three different target tensors to zeros with \n",
    "`targets = [torch.zeros((self.num_anchors // 3, S, S, 6)) for S in self.S]` where `self.S` is a list with the different grid sizes e.g. for an image size of 416x416 we have `S=[13, 26, 52]` or more general we have `S = [image_size// 32, image_size//16, image_size//8]`since at the prediction state the feature map will have be downscaled with the factors in the denominator.\n",
    "\n",
    "The next step is to loop through all the bounding boxes in this particular image. If you have a lot of bounding boxes this will be quite expensive but haven't yet figured out a way to remove this step without taking shortcuts when assigning the anchor boxes. Let me know if you have any ideas on how to optimize this! We will then compute the intersection over union between the target's width and height and all the anchor boxes and sort the result such that the index of the anchor with the largest intersection over union with the target box appears first in the list. \n",
    "```python\n",
    "iou_anchors = iou(torch.tensor(box[2:4]), self.anchors)\n",
    "anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n",
    "```\n",
    "We will then loop through the nine indices to assign the target to the best anchors. Our goal is to assign each target bounding box to an anchor on each scale i.e. in total assign each target to one anchor in each of the target matrices we intialized above. In addition we will also check if an anchor is not the most suitable for the bounding box but it still has an intersection over union higher than 0.5 as is specified in the `ignore_iou_thresh` and then we will mark this target such that no loss is incurred for the prediction of this anchor box. From my understanding the reasoning behind this is that during inference this anchor could also make valid predictions on similar objects and non-max suppression will remove surplus bounding boxes. During training we therefore do not want to force the particular anchor to predict that there is not an object. We first compute which cell the bounding box belongs to by `i, j = int(S * y), int(S * x)` and then we check if the anchor we are currently at is taken in this cell by `anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]`. As you can probably imagine it is relatively uncommon for most datasets to have two objects with midpoint in the same cell of such similar size that they fit the same anchor box, however, if you run this through a couple of hundred examples you'll notice it occurs several times on for example the Pascal-VOC dataset. In addition to checking if the particular anchor is taken, we also check if the current bounding box already has an anchor on this particular prediction scale. We only want one target anchor on each scale to allow for specialization between the anchor boxes such that they focus on prediction different kinds of objects. \n",
    "\n",
    "If we find an anchor that is unoccupied and our current bounding box does not have an anchor on the scale which the anchor belongs to, we want to assign this anchor to the bounding box. First we will set the object score on this anchor to 1 by: `      targets[scale_idx][anchor_on_scale, i, j, 0] = 1,` to indicate that there is an object in this cell. We then compute the box coordinates relative to the cell such the midpoint (x,y) states where in the cell the object is and the width and the height corresponds to how many cells the bounding box covers. This is computed by: \n",
    "```python\n",
    "x_cell, y_cell = S * x - j, S * y - i  # both between [0,1]\n",
    "width_cell, height_cell = width * S, height * S  # can be greater than 1 since it's relative to the cell\n",
    "```\n",
    "We will then add the bounding box coordinates as well as the class label to the cell and the anchor box indicated by `i`, `j` and `anchor_on_scale` respectively. Lastly we will update the flag `has_anchor[scale_idx]` to True to indicate that the particular prediction scale now has an anchor. \n",
    "\n",
    "Only doing the data loading in the way above would be sufficient. In the YOLOv3 paper they, however, also check if the anchor we are currently at has an intersection over union greater than `ignore_iou_thresh = 0.5` and then they do not incur loss for this anchor's prediction. We will do this by setting the object score of the anchor in the object cell to -1 i.e. `targets[scale_idx][anchor_on_scale, i, j, 0] = -1`. In the loss function we will later make sure that no loss is incurred for these anchors. \n",
    "\n",
    "To make sure that the data loading works it is beneficial to plot a few examples with augmentations added to them and the bounding boxes. The code below should do the trick, possibly with some modifications depending on how you structure the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    anchors = config.ANCHORS\n",
    "\n",
    "    transform = config.train_transforms\n",
    "\n",
    "    dataset = YOLODataset(\n",
    "        config.DATASET+'/train',\n",
    "        config.IMG_DIR,\n",
    "        config.LABEL_DIR,\n",
    "        S=[13, 26, 52],\n",
    "        anchors=anchors,\n",
    "        transform=transform,\n",
    "    )\n",
    "    S = [13, 26, 52]\n",
    "    scaled_anchors = torch.tensor(anchors) / (\n",
    "        1 / torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
    "    )\n",
    "    loader = DataLoader(dataset=dataset, batch_size=1, shuffle=True)\n",
    "    for x, y in loader:\n",
    "        boxes = []\n",
    "\n",
    "        for i in range(y[0].shape[1]):\n",
    "            anchor = scaled_anchors[i]\n",
    "            boxes += cells_to_bboxes(\n",
    "                y[i], is_preds=False, S=y[i].shape[2], anchors=anchor\n",
    "            )[0]\n",
    "        boxes = nms(boxes, iou_threshold=1, threshold=0.7, box_format=\"midpoint\")\n",
    "        print(boxes)\n",
    "        plot_image(x[0].permute(1, 2, 0).to(\"cpu\"), boxes)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-houston",
   "metadata": {},
   "source": [
    "## YOLOv3 loss function\n",
    "In the original YOLO paper the author states the loss function and the same expression can be found in articles on YOLOv2 or v3 which is at best a simplification compared to the actual implementation. If you are familiar with the original YOLO loss you will recognize all parts below but they are tweaked to match the idea with the anchor boxes. The loss function can be divided into four parts and I will go through each separately and then combine them in the end. \n",
    "\n",
    "First we will form two binary tensors signifying where in what cells using which anchors that have objects assigned to them and not. \n",
    "```python      \n",
    "obj = target[..., 0] == 1 \n",
    "noobj = target[..., 0] == 0 \n",
    "```\n",
    "The reason for not only using one of these is that we in the data loading assign the anchors which we should ignore to -1. Indexing only the indices above in all parts of the loss function will make sure that we do not incur any loss on these anchors. I will state all parts of the loss also as mathematical formulas based on the way they are implemented in the code. They are just translations from the code for those who find it easier to understand the loss in that format so don't worry if they're not your cup of tea. \n",
    "\n",
    "### No object loss\n",
    "For the anchors in all cells that do not have an object assigned to them i.e. all indices that are set to one in `noobj` we want to incur loss only for their object score. The target will be all zeros since we want these anchors to predict an object score of zero and we will apply a sigmoid function to the network outputs and use a binary crossentropy loss. In code we have that \n",
    "```python \n",
    "no_object_loss = self.bce(\n",
    "                 (predictions[..., 0:1][noobj]), (target[..., 0:1][noobj]),\n",
    "                )\n",
    "```\n",
    "where `self.bce` refers to an instance of the PyTorch BCEWithLogitsLoss() which applies the sigmoid function and then calculates the binary crossentropy loss. \n",
    "\n",
    "In mathematics we have that \n",
    "$$\n",
    "\\begin{aligned} \n",
    "L_{noobj} &= \\frac{1}{N \\sum_{a, i,j} \\mathbb{1}_{a\\ i\\ j}^{\\text {noobj }}} \\sum_{n=1}^N \\sum_{a,i,j \\in \\mathbb{1}_{a\\ i \\ j}^{\\text {noobj }}} BCE \\left ( y_{n,a,i,j}^{obj}, \\sigma\\left(t_{n,a,i,j}^{obj}\\right)\\right) \\\\\n",
    "&=\\frac{1}{N \\sum_{a, i,j} \\mathbb{1}_{a\\ i\\ j}^{\\text {noobj }}} \\sum_{n=1}^N \\sum_{a,i,j \\in \\mathbb{1}_{a\\ i \\ j}^{\\text {noobj }}} -\\left[y_{n,a,i,j}^{obj} \\cdot \\log \\sigma\\left(t_{n,a,i,j}^{obj}\\right)+\\left(1-y_{n,a,i,j}^{obj}\\right) \\cdot \\log \\left(1-\\sigma(t_{n,a,i,j}^{obj})\\right)\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $N$ is the batch size, $i,\\ j$ signifies the cell where and $a$ the anchor index and $\\mathbb{1}_{a\\ i\\ j}^{\\text {noobj }}$ is a binary tensor with ones on anchors not assigned to an object. The output from the network is denoted $t$ and the target $y$ and $\\sigma$ is the sigmoid function given by \n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}}.\n",
    "$$\n",
    "\n",
    "### Object loss\n",
    "For the anchors that have an object assigned to them we want them to predict a appropriate bounding box for the object. When building the target tensors we assigned these anchors to have an object score to 1. One idea is to then just do similarily as in the no object loss and train the network to output large values in the cells and anchors for which we have assigned a target bounding box. This would, however, mean that no matter how horrible a bounding box prediction the network makes it would still try to predict a high object score. During inference we are guided by the object score when choosing which bounding boxes to output and if we do as proposed the object score would actually not reflect how likely it actually is that there is an object in the outputted bounding box. The idea in the YOLOv3 paper instead that the object score that the model predicts should reflect the intersection over union between the prediction and the target bounding box. It is slightly unclear how this is actually implemented originally and I have seen several different versions in others' code. In our implementation we will during training time calculate the intersection over union between the target bounding boxes and the predicted bounding boxes in the output and use this as the target for the object score. This does not seem to slow down training noticeably. \n",
    "\n",
    "In the code we will convert the model predictions to bounding boxes according to the formulas in the paper\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "b_{x}=\\sigma\\left(t_{x}\\right) \\\\\n",
    "b_{y}=\\sigma\\left(t_{y}\\right) \\\\\n",
    "b_{w}=p_{w} e^{t_{w}} \\\\\n",
    "b_{h}=p_{h} e^{t_{h}},\n",
    "\\end{array} \\\\\n",
    "$$\n",
    "where $p_w$ and $p_h$ are the anchor box dimensions and ($b_x, b_y, b_w, b_h$) is the resulting bounding box relative to the cell. We will then calculate the intersection over union with the target that we defined in the dataset class and lastly as in the no object loss above apply the binary cross entropy loss between the object score predictions and the calculated intersection over union. Note that the loss will only be applied to the anchors assigned to a target bounding box signified by indexing by `obj`.  \n",
    "\n",
    "```python\n",
    "anchors = anchors.reshape(1, 3, 1, 1, 2) # reshaping for broadcasting \n",
    "box_preds = torch.cat([self.sigmoid(predictions[..., 1:3]), torch.exp(predictions[..., 3:5]) * anchors], dim=-1)\n",
    "ious = intersection_over_union(box_preds[obj], target[..., 1:5][obj]).detach()\n",
    "object_loss = self.bce((predictions[..., 0:1][obj]), (ious * target[..., 0:1][obj]))\n",
    "```\n",
    "\n",
    "The mathematical formula will be similar to the one above \n",
    "$$\n",
    "L_{obj}= \\frac{1}{N \\sum_{a, i,j} \\mathbb{1}_{a\\ i\\ j}^{\\text {obj }}} \\sum_{n=1}^N \\sum_{a,i,j \\in \\mathbb{1}_{a\\ i \\ j}^{\\text {obj }}} BCE \\left ( \\hat{y}_{n,a,i,j}^{obj}, \\sigma\\left(t_{n,a,i,j}^{obj}\\right)\\right)\n",
    "$$\n",
    "with \n",
    "$$ \\hat{y} = IOU(y^{box}, b) $$\n",
    "where $b$ is the bounding box computed above and $ \\mathbb{1}_{a\\ i\\ j}^{\\text {obj }}$ corresponds to the binary tensor with ones for the anchors assigned to a target bounding box. \n",
    "\n",
    "### Box coordinates loss\n",
    "For the box coordinates we will simply use a mean squared error loss in the positions where there actually are objects. All predictions where there is no corresponding target bounding box will be ignored. We will apply a sigmoid function to the $x$\n",
    "and $y$ coordinates to make sure that they are between \\[0,1\\] but instead of converting the widths and heights as above we want to compute the ground truth value $\\hat{t}$ that the network should predict. We find it by inverting the formula above for the bounding boxes\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\hat{t}_w &= \\log (y_w / p_w) \\\\\n",
    "\\hat{t}_h &= \\log (y_h / p_h)\n",
    "\\end{aligned}\n",
    "$$\n",
    "where the $y_w$ and $y_h$ are the target width and height. We will then apply the mean squared error loss between the targets and predictions.  \n",
    "```python\n",
    "predictions[..., 1:3] = self.sigmoid(predictions[..., 1:3]) # x, y, coordinates\n",
    "target[..., 3:5] = torch.log(1e-16 + target[..., 3:5] / anchors) # convert target width and height\n",
    "box_loss = self.mse(predictions[..., 1:5][obj], target[..., 1:5][obj]) #index by obj to only apply loss for objects\n",
    "```\n",
    "The equivalent formula is given by\n",
    "$$\n",
    "L_{box} = \\frac{1}{N \\sum_{a, i,j} \\mathbb{1}_{a\\ i\\ j}^{\\text {obj }}} \\sum_{n=1}^N \\sum_{a,i,j \\in \\mathbb{1}_{a\\ i \\ j}^{\\text {obj }}}  \\left(\\sigma(t^x_{n,a,i,j}) - y^x_{n,a,i,j} \\right)^2 +\n",
    "\\left(\\sigma(t^y_{n,a,i,j}) - y^y_{n,a,i,j} \\right)^2 +\n",
    "\\left(t^w_{n,a,i,j} - \\hat{t}^w_{n,a,i,j} \\right)^2 +\n",
    "\\left(t^h_{n,a,i,j} - \\hat{t}^h_{n,a,i,j} \\right)^2, \n",
    "$$\n",
    "where $\\hat{t}^*$ is the ground truth labels for what actual values the model should predict.\n",
    "\n",
    "### Class loss\n",
    "We will only incur loss for the class predictions where there actually is an object. Our implementation differs slightly from the paper's in the case of a class loss and we will use a cross entropy loss to compute the class loss. This assumes that each bounding box only has one label. The YOLOv3 motivates that it does not want to have this limitation and instead uses an binary cross entropy such that several labels can be assigned to a single object e.g. woman and person. \n",
    "```python\n",
    "class_loss = self.entropy(predictions[..., 5:][obj], target[..., 5][obj].long()\n",
    "```\n",
    "where `self.entropy` refers to an instance of PyTorch's CrossEntropyLoss() with combines the softmax function and negative loglikelihood loss. This corresponds to\n",
    "\n",
    "$$ \n",
    "L_{class} = \\frac{1}{N \\sum_{a, i,j} \\mathbb{1}_{a\\ i\\ j}^{\\text {obj }}} \\sum_{n=1}^N \\sum_{a,i,j \\in \\mathbb{1}_{a\\ i \\ j}^{\\text {obj }}} -\\log \\left(\\frac{\\exp (t_{n, a, i,j}^{c})}{\\sum_{k} \\exp (t_{n, a, i,j}^k)}\\right),\n",
    "$$\n",
    "where $t_{n, a, i,j}^{c}$ is the prediction for the correct class $c$. \n",
    "\n",
    "### Total loss\n",
    "I will not attempt to put the entire loss function in a single formula as this only creates an unnecessarily complicated expression when each part can be understood and computed separately. The total loss is computed by \n",
    "```python\n",
    "self.lambda_box * box_loss\n",
    "+ self.lambda_obj * object_loss\n",
    "+ self.lambda_noobj * no_object_loss\n",
    "+ self.lambda_class * class_loss\n",
    "```\n",
    "or equivalently \n",
    "$$ L= \\lambda_{noobj} L_{noobj} + \\lambda_{obj} L_{obj} + \\lambda_{box} L_{box} + \\lambda_{class} L_{class} $$\n",
    "where each $\\lambda_*$ is a constant signifying the importance of each part of the loss. It seems that the original implementation uses $\\lambda_* = 1$ for all constants but during training we found better convergence by modifying them. \n",
    "\n",
    "The complete code for the loss function is found below and the code is placed in a separate loss.py file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of Yolo Loss Function similar to the one in Yolov3 paper,\n",
    "the difference from what I can tell is I use CrossEntropy for the classes\n",
    "instead of BinaryCrossEntropy.\n",
    "\"\"\"\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import intersection_over_union\n",
    "\n",
    "\n",
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.entropy = nn.CrossEntropyLoss()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Constants signifying how much to pay for each respective part of the loss\n",
    "        self.lambda_class = 1\n",
    "        self.lambda_noobj = 10\n",
    "        self.lambda_obj = 1\n",
    "        self.lambda_box = 10\n",
    "\n",
    "    def forward(self, predictions, target, anchors):\n",
    "        \"\"\"\n",
    "        :param predictions: output from model of shape: (batch size, anchors on scale , grid size, grid size, 5 + num classes)\n",
    "        :param target: targets on particular scale of shape: (batch size, anchors on scale, grid size, grid size, 6)\n",
    "        :param anchors: anchor boxes on the particular scale of shape (anchors on scale, 2)\n",
    "        :return: returns the loss on the particular scale\n",
    "        \"\"\"\n",
    "\n",
    "        # Check where obj and noobj (we ignore if target == -1)\n",
    "        # Here we check where in the label matrix there is an object or not\n",
    "        obj = target[..., 0] == 1  # in paper this is Iobj_i\n",
    "        noobj = target[..., 0] == 0  # in paper this is Inoobj_i\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "        # The indexing noobj refers to the fact that we only apply the loss where there is no object\n",
    "        no_object_loss = self.bce(\n",
    "            (predictions[..., 0:1][noobj]), (target[..., 0:1][noobj]),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "        # Here we compute the loss for the cells and anchor boxes that contain an object\n",
    "        # Reschape anchors to allow for broadcasting in multiplication below\n",
    "        anchors = anchors.reshape(1, 3, 1, 1, 2)\n",
    "        # Convert outputs from model to bounding boxes according to formulas in paper\n",
    "        box_preds = torch.cat([self.sigmoid(predictions[..., 1:3]), torch.exp(predictions[..., 3:5]) * anchors], dim=-1)\n",
    "        # Targets for the object prediction should be the iou of the predicted bounding box and the target bounding box\n",
    "        ious = intersection_over_union(box_preds[obj], target[..., 1:5][obj]).detach()\n",
    "        # Only incur loss for the cells where there is an objects signified by indexing with obj\n",
    "        object_loss = self.bce((predictions[..., 0:1][obj]), (ious * target[..., 0:1][obj]))\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR BOX COORDINATES    #\n",
    "        # ======================== #\n",
    "        # apply sigmoid to x, y coordinates to convert to bounding boxes\n",
    "        predictions[..., 1:3] = self.sigmoid(predictions[..., 1:3]) \n",
    "        # to improve gradient flow we convert targets' width and height to the same format as predictions\n",
    "        target[..., 3:5] = torch.log(\n",
    "            (1e-16 + target[..., 3:5] / anchors)\n",
    "        ) \n",
    "        # compute mse loss for boxes\n",
    "        box_loss = self.mse(predictions[..., 1:5][obj], target[..., 1:5][obj])\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "        # here we just apply cross entropy loss as is customary with classification problems\n",
    "        class_loss = self.entropy(\n",
    "            (predictions[..., 5:][obj]), (target[..., 5][obj].long()),\n",
    "        )\n",
    "        \n",
    "        return (\n",
    "            self.lambda_box * box_loss\n",
    "            + self.lambda_obj * object_loss\n",
    "            + self.lambda_noobj * no_object_loss\n",
    "            + self.lambda_class * class_loss\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-chicago",
   "metadata": {},
   "source": [
    "## Training the model \n",
    "The training configuration is completely contained in the config.py file that can be found on [Github](https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/object_detection/YOLOv3). This is where we specify the image size, dataset paths, augmentations, learning rate and all other constants. I will not include it here and if you implement YOLOv3 you can just copy it from above or write you own training configuration. \n",
    "\n",
    "What we instead will focus on is building the training loop which should be quite straightforward. Everything from here will be placed in a train.py file which we can then run to train the model. First we will define the imports where we will import our previously defined modules and in addition a couple of helper functions from the utils.py file you can find on [Github](https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/object_detection/YOLOv3/utils.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from model import YOLOv3\n",
    "from tqdm import tqdm\n",
    "from utils import (\n",
    "    mean_average_precision,\n",
    "    cells_to_bboxes,\n",
    "    get_evaluation_bboxes,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    "    check_class_accuracy,\n",
    "    get_loaders,\n",
    "    plot_couple_examples\n",
    ")\n",
    "from loss import YoloLoss\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-hardwood",
   "metadata": {},
   "source": [
    "We will then define a training function which will train the network for one epoch. We will take as input the model, the data loader, the optimizer the loss function, a scaler for mixed precision training and scaled anchors such that each anchor is relative to the prediction scale. Originally the anchors are relative to the entire image but to the loss we want to input them relative to the cell and this is accomplished by scaling them with the grid size of the prediction scale.    \n",
    "\n",
    "We calculate the total loss as the sum of the losses for each prediction scale, three of them in total. We use mixed precision training to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    losses = []\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x = x.to(config.DEVICE)\n",
    "        y0, y1, y2 = (\n",
    "            y[0].to(config.DEVICE),\n",
    "            y[1].to(config.DEVICE),\n",
    "            y[2].to(config.DEVICE),\n",
    "        )\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(x)\n",
    "            loss = (\n",
    "                loss_fn(out[0], y0, scaled_anchors[0])\n",
    "                + loss_fn(out[1], y1, scaled_anchors[1])\n",
    "                + loss_fn(out[2], y2, scaled_anchors[2])\n",
    "            )\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # update progress bar\n",
    "        mean_loss = sum(losses) / len(losses)\n",
    "        loop.set_postfix(loss=mean_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-force",
   "metadata": {},
   "source": [
    "We have now come to the part where we are ready to actually train the model. The main function will take care of setting up the model, loss function, data loaders etc. and in each epoch we will run the train function defined above. Once every ten epochs we will evaluate the model by checking the mean average precision on the test loader. Note that this can be costly if your model's performance is bad because there may be many false positives that the non max suppression and mean average precision functions have to loop through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model = YOLOv3(num_classes=config.NUM_CLASSES).to(config.DEVICE)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY\n",
    "    )\n",
    "    loss_fn = YoloLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    train_loader, test_loader, train_eval_loader = get_loaders(\n",
    "        train_csv_path=config.DATASET + \"/train.csv\", test_csv_path=config.DATASET + \"/test.csv\"\n",
    "    )\n",
    "\n",
    "    if config.LOAD_MODEL:\n",
    "        load_checkpoint(\n",
    "            config.CHECKPOINT_FILE, model, optimizer, config.LEARNING_RATE\n",
    "        )\n",
    "        \n",
    "    #Scale anchors to each prediction scale\n",
    "    scaled_anchors = (\n",
    "        torch.tensor(config.ANCHORS)\n",
    "        * torch.tensor(config.S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
    "    ).to(config.DEVICE)\n",
    "\n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors)\n",
    "\n",
    "        if config.SAVE_MODEL:\n",
    "            save_checkpoint(model, optimizer, filename=f\"checkpoint.pth.tar\")\n",
    "\n",
    "        if epoch % 10 == 0 and epoch > 0:\n",
    "            print(\"On Test loader:\")\n",
    "            check_class_accuracy(model, test_loader, threshold=config.CONF_THRESHOLD)\n",
    "            # Run model on test set and convert outputs to bounding boxes relative to image\n",
    "            pred_boxes, true_boxes = get_evaluation_bboxes(\n",
    "                test_loader,\n",
    "                model,\n",
    "                iou_threshold=config.NMS_IOU_THRESH,\n",
    "                anchors=config.ANCHORS,\n",
    "                threshold=config.CONF_THRESHOLD,\n",
    "            )\n",
    "            # Compute mean average precision \n",
    "            mapval = mean_average_precision(\n",
    "                pred_boxes,\n",
    "                true_boxes,\n",
    "                iou_threshold=config.MAP_IOU_THRESH,\n",
    "                box_format=\"midpoint\",\n",
    "                num_classes=config.NUM_CLASSES,\n",
    "            )\n",
    "            print(f\"MAP: {mapval.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-psychology",
   "metadata": {},
   "source": [
    "We have now reached the end of this YOLOv3 implementation and if you feel that everything is crystal clear then: Wow I've really outdone myself. It is more likely that you have to reiterate this and possibly others' implementations if your goal is to implement YOLOv3 yourself. Anyhow, I hope that you take with you some key implementational details of YOLOv3 from this article and if you have any lingering thoughts, leave a comment!  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
